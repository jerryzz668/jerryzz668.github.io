<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>小白也能看懂的BP反向传播</title>
    <url>/jerryzz668.github.io/2020/03/01/%E5%B0%8F%E7%99%BD%E4%B9%9F%E8%83%BD%E7%9C%8B%E6%87%82%E7%9A%84BP%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
    <content><![CDATA[<h4 id="BP过程中，更新公式长什么样？"><a href="#BP过程中，更新公式长什么样？" class="headerlink" title="BP过程中，更新公式长什么样？"></a>BP过程中，更新公式长什么样？</h4><p>众所周知，BP过程中通过权值的不断更新，使得神经网络的输出向groundtruth靠近。</p>
<a id="more"></a>
<p>开门见山，直接看图，这部分是为==后面==做铺垫，很简单昂~</p>
<p><img src="https://s2.ax1x.com/2020/03/01/3g4ZNR.png" alt="1"></p>
<p>我们随机给输入a，b添加一个随机值，然后用步长step来控制增加速度，看python代码：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">def multiply(a, b):</span><br><span class="line">    <span class="built_in">return</span> a * b</span><br><span class="line"></span><br><span class="line">def func():</span><br><span class="line">    a = 5</span><br><span class="line">    b = 6</span><br><span class="line">    step = 0.01</span><br><span class="line">    a = a + step * (random.random())</span><br><span class="line">    b = b + step * (random.random())</span><br><span class="line">    <span class="built_in">print</span>(multiply(a, b))</span><br><span class="line"></span><br><span class="line">func()</span><br></pre></td></tr></table></figure>
<p>运行程序，输出30.023996846498836，比30大，但是如果继续测试，会出现问题，速看如下代码：</p>
<figure class="highlight autoit"><table><tr><td class="code"><pre><span class="line">def <span class="function"><span class="keyword">func</span><span class="params">()</span>:</span></span><br><span class="line">    a = <span class="number">-5</span></span><br><span class="line">    b = <span class="number">-6</span></span><br><span class="line">    <span class="keyword">step</span> = <span class="number">0.01</span></span><br><span class="line">    a = a + <span class="keyword">step</span> * (<span class="built_in">random</span>.<span class="built_in">random</span>())</span><br><span class="line">    b = b + <span class="keyword">step</span> * (<span class="built_in">random</span>.<span class="built_in">random</span>())</span><br><span class="line">    print(multiply(a, b))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<p>运行程序，输出29.982885896252554，比30小，把输入a，b改成负数就失效了，所以对于step后面乘的系数，应该找一个和输入相关的值来控制，也就是一个关于输入值的函数。如果在这个点上，输出结果是随着输入值增加而增加，那么step乘以一个正数即可，反之，则需要乘以一个负数才能使输出结果增加！显然这就是函数在某点上的==微分==（前文铺垫的就是他）的定义！</p>
<p>eg：对于f = f(a, b) ,我们想要更新变量a，b的值，就可以根据下图公式来更新</p>
<p><img src="https://s2.ax1x.com/2020/03/01/3g4n9x.png" alt="2"></p>
<p>实际上这就是BP的最基本思想啦~假设f就是神经网络的输出，将f与groundtruth计算误差，让f变大（或变小），使其更接近groundtruth，我们将改变f的变量，而f其实是长这样的，f = f(w,b) . 只不过真正的神经网络更复杂。</p>
<h4 id="进一步了解BP"><a href="#进一步了解BP" class="headerlink" title="进一步了解BP"></a>进一步了解BP</h4><p>接下来，我们将一步一步地接近最终的神经网络中的BP。</p>
<p>不废话，直接看图。</p>
<p><img src="https://s2.ax1x.com/2020/03/01/3g41De.png" alt="3"></p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiply</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * y </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addition</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x + y</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(a, b, c)</span>:</span></span><br><span class="line">    d = addition(a, b)</span><br><span class="line">    <span class="keyword">return</span> multiply(d, c)</span><br><span class="line"> </span><br><span class="line">print(forward(<span class="number">5</span>, <span class="number">-6</span>, <span class="number">7</span>))</span><br></pre></td></tr></table></figure>
<p>输出结果为-7，现在开始训练，就是改变输入的a,b,c三个值，使函数f的值变大，那么就按照下图的方式更新。</p>
<h5 id="更新公式"><a href="#更新公式" class="headerlink" title="更新公式"></a>更新公式</h5><p><img src="https://s2.ax1x.com/2020/03/01/3g4Y4I.png" alt="4"></p>
<h5 id="链式求导"><a href="#链式求导" class="headerlink" title="链式求导"></a>链式求导</h5><p>现在问题就变成了如何求解f关于a，b，c的微分。<br>我们将f反着往回写，我们将第一个神经元的输出记作d，那么f = d *c , 继续反着推，d = a +b。这样就成了两个函数。如图：</p>
<p><img src="https://s2.ax1x.com/2020/03/01/3g4NCt.png" alt="5"></p>
<p>下面开始链式求导，考过数学一二三的人最喜欢这个东西了。</p>
<p><img src="https://s2.ax1x.com/2020/03/01/3g4agf.png" alt="6"></p>
<p>继续，==不要停==：</p>
<p><img src="https://s2.ax1x.com/2020/03/01/3g4BDg.png" alt="7"></p>
<p>然后最终结果如下：</p>
<p><img src="https://s2.ax1x.com/2020/03/01/3g4R2V.png" alt="8"></p>
<h5 id="反向传播代码实现"><a href="#反向传播代码实现" class="headerlink" title="反向传播代码实现"></a>反向传播代码实现</h5><p>代码实现上述过程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiply</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * y</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addition</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x + y</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(a, b, c)</span>:</span></span><br><span class="line">    d = addition(a, b)</span><br><span class="line">    <span class="keyword">return</span> multiply(d, c)</span><br><span class="line">print(forward(<span class="number">5</span>, <span class="number">-6</span>, <span class="number">7</span>))</span><br></pre></td></tr></table></figure>
<p>输出结果为-7</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">def</span> <span class="string">update(a, b, c):</span></span><br><span class="line">    <span class="attr">d</span> = <span class="string">addition(a, b)</span></span><br><span class="line">    <span class="attr">h</span> = <span class="string">0.01</span></span><br><span class="line"> </span><br><span class="line">    <span class="attr">derivative_f_d</span> = <span class="string">c</span></span><br><span class="line">    <span class="attr">derivative_f_c</span> = <span class="string">d</span></span><br><span class="line">    <span class="attr">derivative_d_a</span> = <span class="string">1</span></span><br><span class="line">    <span class="attr">derivative_d_b</span> = <span class="string">1</span></span><br><span class="line"> </span><br><span class="line">    <span class="attr">derivative_f_a</span> = <span class="string">derivative_f_d * derivative_d_a</span></span><br><span class="line">    <span class="attr">derivative_f_b</span> = <span class="string">derivative_f_d * derivative_d_b</span></span><br><span class="line"> </span><br><span class="line">    <span class="attr">a</span> = <span class="string">a + h * derivative_f_a</span></span><br><span class="line">    <span class="attr">b</span> = <span class="string">b + h * derivative_f_b</span></span><br><span class="line">    <span class="attr">c</span> = <span class="string">c + h * derivative_f_c</span></span><br><span class="line"> </span><br><span class="line">    <span class="attr">d</span> = <span class="string">addition(a, b)</span></span><br><span class="line">    <span class="attr">return</span> <span class="string">multiply(d, c)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">print(update(5,</span> <span class="string">-6, 7))</span></span><br></pre></td></tr></table></figure>
<p>输出结果为-6.0113999999999965<br>可以看到更新之后的输出确实增大了,说明我们现在已经可以实现嵌套神经元的变量参数的更新了。</p>
<h4 id="总结BP更新本质"><a href="#总结BP更新本质" class="headerlink" title="总结BP更新本质"></a>总结BP更新本质</h4><p>首先，前向传播，从输入开始分析，输入值为a=5,b=-6,c=7，第一个神经元的输出d为-1，然后第二个神经元输出为7，最后结果为-7，这就是前向传播的过程。</p>
<p>然后，反向传播，从输出开始分析，目标是将输出变大，输出值是由-1*7得到的，现在要增加输出值，显然就是增加-1，减少7。再来计算微分，微分结果就是增加d，减少c。这里d又是由a，b决定的！那么只要增加a和b就行了，运用链式法则求a，b的微分，也能得到相同的结果。</p>
<p>我们倒着从输出到输入的过程就是反向传播的过程。通过计算微分可以更新变量的值，使得输出朝着我们期待的方向的变化！</p>
<h4 id="好吃又简单的BP，到这里你就懂啦"><a href="#好吃又简单的BP，到这里你就懂啦" class="headerlink" title="好吃又简单的BP，到这里你就懂啦~"></a>好吃又简单的BP，到这里你就懂啦~</h4>]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>BP</tag>
      </tags>
  </entry>
  <entry>
    <title>卷积神经网络(CNN)辅助入门理解</title>
    <url>/jerryzz668.github.io/2020/01/28/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)%E8%BE%85%E5%8A%A9%E5%85%A5%E9%97%A8%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<p>如果开始看CNN了，相信大家对于神经网络的基础已经有了一些了解了，那么今天我们就来感性的理解下CNN，尽可能少整数学公式，让在座各位兄弟在学习工作之余，嗑个瓜子，吃根冰棍的同时也能看懂CNN，但是在编程过程中，数学公式可是必不可少的嗷～</p>
<a id="more"></a>
<h4 id="CNN简介"><a href="#CNN简介" class="headerlink" title="CNN简介"></a>CNN简介</h4><p>这个CNN，不是美国有线电视新闻网，是卷积神经网络(Convolutional Neural Networks)。此概念在195x年被两个研究小猫大脑的美国老铁提出，他们发现这种结构可以极大简化小猫识别物体所需要的神经网络结构。随着近些年深度学习的火热，CNN也得到了广泛的应用。今天的任务是：知道CNN每一步工作原理，为什么CNN简化了神经网络，以及CNN进一步的思考。</p>
<p>首先来看下卷积是啥，提起卷积，我脑中莫名会想起小时候吃的大大卷<br><img src="https://s2.ax1x.com/2020/01/28/1K34PJ.jpg" alt="大大卷儿"><br>其实呢，卷积就是两个变量在某范围内相乘后求和的结果。</p>
<p>一开始看CNN，我迷茫地像个两百多斤的孩子一样，机械地接受着知识，神马卷积层池化层，一层层叠，然后又来几个全连接层，然后，然后好啦？对啊，我知道池化卷积都是干什么的，可是真正一步步的实现是怎样呢？高大上的论文，教程都不详细说明，仿佛受众都是深度学习老司机一样。当时越看越不安，得好好思考下为什么这么设计网络结构。</p>
<p>那么先来一个CNN可视化图压压惊<br><img src="https://s2.ax1x.com/2020/01/28/1K8ViQ.png" alt="可视化图"><br>这是一个训练好的CNN网络，最下面的输入层是我写的一个丑陋的2，后面经过卷积，池化，第二次卷积，池化，最后来两个全连接，输出结果</p>
<p>你也可以去这个链接玩玩<a href="https://tensorspace.org/html/playground/lenet.html" target="_blank" rel="noopener">CNN-3D模型可视化</a></p>
<h4 id="感性认知CNN"><a href="#感性认知CNN" class="headerlink" title="感性认知CNN"></a>感性认知CNN</h4><p>下面用一只活生生的例子来让大家感性地认知下CNN工作原理，我觉得在看以下内容之前，你可以去教学材料了解下CNN那些构造的基本作用，只知道如何操作即可，这对感性理解会有较大帮助。<br>首先我们选四个input送进一个训练好的CNN网络，分别是豹子，豹纹钱包，织物钱包，橙色皮钱包：<br><img src="https://s2.ax1x.com/2020/01/28/1K8yJH.jpg" alt="input"></p>
<h5 id="第一次卷积"><a href="#第一次卷积" class="headerlink" title="第一次卷积"></a>第一次卷积</h5><p>首先这些input会通过第一层卷积层，也就是滤波器，所谓的kernal，就张这个样子<br><img src="https://s2.ax1x.com/2020/01/28/1K86Wd.jpg" alt="kernal"><br>下面就用这个滤波器去扫描每张图片3<em>3区域，从头扫到尾滴水不漏的扫描，当然你可以规定每次相邻扫描的重叠部分，也就是用步长（stride）去控制，我们这个滤波器是3</em>3，所以可选择每次向相邻位置挪动1，2或者3个格子。Look at the kernal，给了左下到右上对角线上赋予权值是正，其余是负，所以每次扫描中，被扫描部分的这个对角线区域就会保留，剩下的都会变成负值，扫描过后会生成一个特征图，就像这样（下图这个例子是5*5的滤波器）<br><img src="https://s2.ax1x.com/2020/01/28/1K8qln.png" alt="feature"><br>那么我扫完这个图了，如果这个图有很多斜向右的微小结构，那么生成的相应特征图就会有很多正值。用这个滤波器去扫描那个豹子，很有可能得出的特征图是一大片负数值（我们假设豹子皮表面不存在这样的微小结构）。去扫那个织物钱包很有可能就得到充满正值的特征图（假设织物一针一针的呈现X）那么咋识别出豹子皮捏？<br>要知道这是一个训练好的网络，滤波器应有尽有，所以啥小特征都给你搞定，网络又拿出一堆滤波器（照妖镜）去找豹子皮<br><img src="https://s2.ax1x.com/2020/01/28/1KGilR.jpg" alt="zhaoyaojing"><br>有识别左上右下斜线的，有识别人字形的，有C字形的<br>我们假设豹子表面是这样的，别问我为什么，强行想象一根根豹毛就是这样的<br><img src="https://s2.ax1x.com/2020/01/28/1KGZTO.jpg" alt="baozimao"><br>哇，豹子的识别出来了！胜利迈出第一步，淘汰掉织物钱包<br>然而，你会发现，豹子，豹纹钱包和橙色豹皮钱包生成的特征图都有很多正值，就像这样<br><img src="https://s2.ax1x.com/2020/01/28/1KG0cn.png" alt="feature map"><br>大致可以知道哪个对哪个，但是计算机不知道啊，计算机只知道，这几张图有很多正值，都有成为豹子的可能。</p>
<h5 id="第一次池化"><a href="#第一次池化" class="headerlink" title="第一次池化"></a>第一次池化</h5><p>我们看那些特征图，每一个最根正苗红的正值像素后面都是一个“人”结构，所以说，每个像素都蕴含了更多的信息。那么为了简化网络结构，我们可以对特征图进行压缩了，这就是池化。如图所示，把每四个相邻的像素选出一个最大值生成新的图像。就像人民代表制度一样，你这个四口之家有根正苗红的红二代么？有请把他叫出来并附上他的身份证明。什么？你家四口都是负值的反动派？矬子里拔将军吧，把最不反动的那位拎出来。每个家庭选出最厉害的一个人进入下一关。经过如此一番折腾，图片尺寸减半<br><img src="https://s2.ax1x.com/2020/01/28/1KGO9H.png" alt="chihua"></p>
<h5 id="第二次卷积池化"><a href="#第二次卷积池化" class="headerlink" title="第二次卷积池化"></a>第二次卷积池化</h5><p>后面进入第二层卷积(为了简洁，第二次卷积环节同第一次，略)<br>这次，为了把豹子搞出来，电脑又拿出一堆滤波器，还是3<em>3，你会问，咋还是3</em>3，说好的检查更复杂程序呢？别急啊，你看，同样是“人”字形滤波器，这次如果你和周围的像素兄弟点亮这个滤波器，会是什么情况？如图所示<br><img src="https://s2.ax1x.com/2020/01/28/1KJSDP.jpg" alt="filter"><br>你们得有这样的阵型，小人组的大人才能点亮滤波器，因为一个根正苗红的像素代表的是他背后符合上一次滤波器检验的周围邻里街坊啊。<br>所以呢，同样是3*3滤波器，检测出来的内容更加复杂了。言归正传，这次电脑要从豹纹这个环节来鉴别这些图片，于是电脑祭出这个滤波器<br><img src="https://s2.ax1x.com/2020/01/28/1KJi4g.jpg" alt="filter"><br>这个可以检测出弧形，把这个来滤波器旋转就可检测不同方向的弧形了，经过一番操作后，由毛皮材质组成的弧形搞出来了</p>
<p>由于橙色毛皮钱包没有纹路，在这轮卷积败下阵来</p>
<p>后面的过程其实也可省略掉了，后面电脑又搞出一些奇技淫巧弄出了豹纹钱包和豹子之间的区别，比如豹子有耳朵钱包没有等等，当然我这个描述中省略了相当的细节，但是对于理解应该会有帮助吧。</p>
<p>下面两图分别是CNN和ACGAN网络<br><img src="https://s2.ax1x.com/2020/01/28/1KJLZV.png" alt="CNN"><br><img src="https://s2.ax1x.com/2020/01/28/1KY6W4.png" alt="ACGAN"></p>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><h5 id="TFboys-MNIST走起来"><a href="#TFboys-MNIST走起来" class="headerlink" title="TFboys+MNIST走起来"></a>TFboys+MNIST走起来</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br></pre></td></tr></table></figure>
<h5 id="定义几个函数-权值，偏置，卷积层，池化层"><a href="#定义几个函数-权值，偏置，卷积层，池化层" class="headerlink" title="定义几个函数 权值，偏置，卷积层，池化层"></a>定义几个函数 权值，偏置，卷积层，池化层</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mnist= input_data.read_data_sets(<span class="string">"MNIST_data"</span>,one_hot=True)</span><br><span class="line">batch_size=100</span><br><span class="line">n_batch = mnist.train.num_examples//batch_size</span><br><span class="line"></span><br><span class="line">def weight_variable(shape):</span><br><span class="line">    initial=tf.truncated_normal(shape,stddev=0.1)</span><br><span class="line">    <span class="built_in">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">def bias_variable(shape):</span><br><span class="line">    initial=tf.constant(0.1,shape=shape)</span><br><span class="line">    <span class="built_in">return</span> tf.Variable(initial)</span><br><span class="line">//CNN</span><br><span class="line">def conv2d(x,W):</span><br><span class="line">    //input tensor of shape [batch, in_height, in_width,in_channels]</span><br><span class="line">    //W filter /kernel tensor of shape [filter_height,filter_width.in_hannels,out_channels]</span><br><span class="line">    //strides[0]=stride[3]=1, stride[1]=x方向步长，stride[2]代表y方向步长</span><br><span class="line">    //padding:A string from: <span class="string">"same"</span>,<span class="string">"valid"</span></span><br><span class="line">    <span class="built_in">return</span> tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line">def max_pool_2x2(x):</span><br><span class="line">    //ksize [1,x,y,1]</span><br><span class="line"><span class="built_in">return</span> tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding=<span class="string">"SAME"</span>)</span><br></pre></td></tr></table></figure>
<h5 id="构造网络结构"><a href="#构造网络结构" class="headerlink" title="构造网络结构"></a>构造网络结构</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">x=tf.placeholder(tf.float32,[None,784])//28*28</span><br><span class="line">y=tf.placeholder(tf.float32,[None,10])</span><br><span class="line">//改变x的格式转为4D向量[batch,in_height,in_width,in_channels]</span><br><span class="line">x_image=tf.reshape(x,[-1,28,28,1])</span><br><span class="line">//初始化第一个卷积层的权值和偏置</span><br><span class="line">W_conv1=weight_variable([5,5,1,32]) //5*5采样窗口，32个卷积核从一个平面抽取特征</span><br><span class="line">b_conv1=bias_variable([32])//每一个卷积核一个偏置值</span><br><span class="line">//把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数</span><br><span class="line">h_conv1=tf.nn.relu(conv2d(x_image,W_conv1)+ b_conv1)</span><br><span class="line">h_pool1=max_pool_2x2(h_conv1) //进行max-pooling</span><br><span class="line">//初始化第二个卷积层的权值和偏置</span><br><span class="line">W_conv2=weight_variable([5,5,32,64])//5*5采样窗口，64个卷积核从32个平面抽取特征</span><br><span class="line">b_conv2=bias_variable([64])</span><br><span class="line"></span><br><span class="line">h_conv2=tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)</span><br><span class="line">h_pool2=max_pool_2x2(h_conv2)</span><br><span class="line">//28*28的图片第一次卷积后还是28*28，第一次赤化后变为14*14，第二次赤化后为7*7</span><br><span class="line">//经过上面操作后得到64张7*7的平面</span><br><span class="line">//初始化第一个全连接层的权侄</span><br><span class="line">W_fc1 = weight_variable([7*7*64,1024])</span><br><span class="line">//上一层有7*7*64个神经元，全连接层有1024个神经元</span><br><span class="line">b_fc1=bias_variable([1024])</span><br><span class="line"></span><br><span class="line">//把池化层2的输出扁平化为1 维</span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])</span><br><span class="line">//求第一个全连接层的输出</span><br><span class="line">h_fc1=tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)</span><br><span class="line">//keep_prob用来表示神经元的输出概率</span><br><span class="line">keep_prob=tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop=tf.nn.dropout(h_fc1,keep_prob)</span><br><span class="line"></span><br><span class="line">//初始化第二个全连接层</span><br><span class="line">W_fc2=weight_variable([1024,10])</span><br><span class="line">b_fc2=bias_variable([10])</span><br><span class="line">//计算输出</span><br><span class="line">prediction=tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)</span><br><span class="line">//交叉熵代价函数</span><br><span class="line">cross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))</span><br><span class="line"></span><br><span class="line">train_step=tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</span><br><span class="line">correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))</span><br><span class="line">accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br></pre></td></tr></table></figure>
<h5 id="执行训练，例行公事"><a href="#执行训练，例行公事" class="headerlink" title="执行训练，例行公事"></a>执行训练，例行公事</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(21):</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> range(n_batch):</span><br><span class="line">            batch_xs,batch_ys =mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:0.7&#125;)</span><br><span class="line">        acc=sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'iter'</span>+str(epoch)+<span class="string">", test acc ="</span>+str(acc))</span><br></pre></td></tr></table></figure>

<p>简单好吃，物美价廉</p>
<h4 id="以上是比较简单的CNN网络，如果想进阶，推荐如下"><a href="#以上是比较简单的CNN网络，如果想进阶，推荐如下" class="headerlink" title="以上是比较简单的CNN网络，如果想进阶，推荐如下"></a>以上是比较简单的CNN网络，如果想进阶，推荐如下</h4><p>论文Gradient-Based Learning Applied to Document Recognition[J]。</p>
]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>Tensorlayer</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>日常碰到的小bug(不定期更新)</title>
    <url>/jerryzz668.github.io/2020/01/21/%E6%97%A5%E5%B8%B8%E7%A2%B0%E5%88%B0%E7%9A%84%E5%B0%8Fbug(%E4%B8%8D%E5%AE%9A%E6%9C%9F%E6%9B%B4%E6%96%B0)/</url>
    <content><![CDATA[<p>作为一个日常踩坑的程序员，避免不了经常性的遇到同样的小bug，不定期记录自己的踩坑时刻。</p>
<a id="more"></a>
<h3 id="解决error-mounting-dev-sdb1-at-media-command-line-exfat-问题"><a href="#解决error-mounting-dev-sdb1-at-media-command-line-exfat-问题" class="headerlink" title="解决error mounting dev sdb1 at media command-line exfat 问题"></a>解决error mounting dev sdb1 at media command-line exfat 问题</h3><p>在Ubuntu16.04插入U盘，无法识别，并弹出如下提示框：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">error mounting ... dev sdb1 at ... media <span class="built_in">command</span>-line ... exfat</span><br></pre></td></tr></table></figure>
<p>解决办法：<br>安装如下包：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install exfat-fuse exfat-utils</span><br></pre></td></tr></table></figure>
<p>一般运行完上述命令，拔出U盘重新插入，就可以了</p>
]]></content>
      <categories>
        <category>technology</category>
      </categories>
      <tags>
        <tag>bug</tag>
        <tag>日常</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu16.04下安装cuda和cudnn(亲测有效)</title>
    <url>/jerryzz668.github.io/2020/01/21/Ubuntu16.04%E4%B8%8B%E5%AE%89%E8%A3%85cuda%E5%92%8Ccudnn(%E4%BA%B2%E6%B5%8B%E6%9C%89%E6%95%88)/</url>
    <content><![CDATA[<p>上一篇博客讲述了Ubuntu16.04下如何安装显卡驱动，这篇继续讲述如何安装cuda和cudnn。首先安装之前首先要确认你需要安装的cuda和cudnn版本<a id="more"></a></p>
<h3 id="cuda的安装"><a href="#cuda的安装" class="headerlink" title="cuda的安装"></a>cuda的安装</h3><p>1.下载<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">cuda安装文件</a>。按照你的系统配置选择安装包，例如我安装的是cuda9.0，那么选择Linux，x86_64，Ubuntu，16.04，最后一个Installer type 选择runfile(local)或者deb(local)都可以，我选择的是deb(local)，下面会分别给出相应的安装过程。<br>2.1 deb(local)安装<br>进入到deb(local)位置，我的是放在Download路径下，在此路径下打开终端，输入</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo dpkg -I cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y cuda</span><br></pre></td></tr></table></figure>
<p>2.2 安装完毕之后，设置环境变量。打开bashrc文件，将cuda路径写入</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo vim ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>按i进行编辑，把以下3个路径粘贴到bashrc文件中</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$LD_LIBRARY_PATH</span>:/usr/<span class="built_in">local</span>/cuda-9.0/lib64</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/usr/<span class="built_in">local</span>/cuda-9.0/bin</span><br><span class="line"><span class="built_in">export</span> CUDA_HOME=<span class="variable">$CUDA_HOME</span>:/usr/<span class="built_in">local</span>/cuda-9.0</span><br></pre></td></tr></table></figure>
<p>然后友情提示：输入<code>:wq</code>,回车即可完成添加路径。<br>终端运行：<code>source ~/.bashrc</code><br>查看cuda 版本<br>输入<code>nvcc –-version</code><br>3.1 runfile(local)安装<br>进入到runfile(local)位置，我的是放在Download路径下，在此路径下打开终端，输入</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo sh cuda_9.0.176_384.81_linux.run</span><br></pre></td></tr></table></figure>
<p>然后找个按住回车键不撒手，直到服务条款显示到100%，然后按照下面进行选择：accept，n(不要安装driver)，y，y，y，<br>安装完成后，设置环境变量，方法同deb(local)安装</p>
<h3 id="cudnn的安装"><a href="#cudnn的安装" class="headerlink" title="cudnn的安装"></a>cudnn的安装</h3><p>1.下载对应的<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">安装文件</a><br>2.安装cudnn<br>解压刚下载的文件，出现cuda文件夹，当前路径(我的路径为Download)打开终端</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo cp cuda/include/cudnn.h /usr/<span class="built_in">local</span>/cuda/include/ </span><br><span class="line">sudo cp cuda/lib64/libcudnn* /usr/<span class="built_in">local</span>/cuda/lib64/</span><br><span class="line">sudo chmod a+r /usr/<span class="built_in">local</span>/cuda/include/cudnn.h</span><br><span class="line">sudo chmod a+r /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure>
<p>3.查看cudnn版本<br>终端输入</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /usr/<span class="built_in">local</span>/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</span><br></pre></td></tr></table></figure>
<p>看到红白相间的版本信息，即安装成功。</p>
<h3 id="安装完成，快去感受一下你的算力吧"><a href="#安装完成，快去感受一下你的算力吧" class="headerlink" title="安装完成，快去感受一下你的算力吧"></a>安装完成，快去感受一下你的算力吧</h3>]]></content>
      <categories>
        <category>system</category>
      </categories>
      <tags>
        <tag>cuda</tag>
        <tag>cudnn</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu16.04下安装NVIDIA显卡驱动</title>
    <url>/jerryzz668.github.io/2020/01/17/Ubuntu16.04%E4%B8%8B%E5%AE%89%E8%A3%85NVIDIA%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8/</url>
    <content><![CDATA[<p>Ubuntu16.04默认安装了第三方开源的驱动程序nouveau，安装nvidia显卡驱动首先需要禁用nouveau，不然会碰到冲突的问题，导致无法安装nvidia显卡驱动。<a id="more"></a></p>
<h4 id="第一步-禁用nouveau"><a href="#第一步-禁用nouveau" class="headerlink" title="第一步, 禁用nouveau"></a>第一步, 禁用nouveau</h4><ol>
<li>编辑文件blacklist.conf <code>sudo vim /etc/modprobe.d/blacklist.conf</code></li>
<li>若未安装vim, 则<code>sudo apt-get install vim</code>安装</li>
<li>在文件最后部分插入以下两行内容<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset=0</span><br></pre></td></tr></table></figure></li>
<li>更新系统<code>sudo update-initramfs -u</code></li>
<li>重启系统（一定要重启）</li>
<li>验证nouveau是否已禁用<code>lsmod | grep nouveau</code><br>没有信息显示，说明nouveau已被禁用，接下来可以安装nvidia的显卡驱动。</li>
</ol>
<hr>
<h4 id="第二步-在英伟达的官网上查找你自己电脑的显卡型号然后下载相应的驱动。网址：NVIDIA官网"><a href="#第二步-在英伟达的官网上查找你自己电脑的显卡型号然后下载相应的驱动。网址：NVIDIA官网" class="headerlink" title="第二步, 在英伟达的官网上查找你自己电脑的显卡型号然后下载相应的驱动。网址：NVIDIA官网"></a>第二步, 在英伟达的官网上查找你自己电脑的显卡型号然后下载相应的驱动。网址：<a href="http://www.nvidia.cn/page/home.html" target="_blank" rel="noopener">NVIDIA官网</a></h4><p>我下载的版本：NVIDIA-Linux-x86_64-430.50.run（注意不同的版本最后安装执行的具体选项不同）</p>
<ol>
<li>下载后的run文件拷贝至home目录下</li>
<li>在ubuntu下按ctrl+alt+f1进入命令行界面，关闭图形界面，不执行会出错<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo service lightdm stop</span><br></pre></td></tr></table></figure></li>
<li>然后卸载掉原有驱动(若安装过其他版本或其他方式安装过驱动执行此)<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get remove nvidia-*</span><br></pre></td></tr></table></figure></li>
<li>给驱动run文件赋予执行权限：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo chmod  a+x NVIDIA-Linux-x86_64-430.50.run</span><br></pre></td></tr></table></figure></li>
<li>安装：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo ./NVIDIA-Linux-x86_64-430.50.run -no-x-check -no-nouveau-check -no-opengl-files</span><br></pre></td></tr></table></figure>
//只有禁用opengl这样安装才不会出现循环登陆的问题<br>//-no-x-check：安装驱动时关闭X服务<br>//-no-nouveau-check：安装驱动时禁用nouveau<br>//-no-opengl-files：只安装驱动文件，不安装OpenGL文件</li>
<li>安装过程中的选项：（这是copy别人的，自己的没记住，我也是尝试选择了好多遍才安装好）</li>
</ol>
<p>Q:The distribution-provided pre-install script failed! Are you sure you want to continue? 选择 yes 继续。<br>Q:Would you like to register the kernel module souces with DKMS? This will allow DKMS to automatically build a new module, if you install a different kernel later?  选择 No 继续。<br>Q：xxx？ 选择install without signing<br>Q:Nvidia’s 32-bit compatibility libraries? 选择 No 继续。<br>Q:Would you like to run the nvidia-xconfigutility to automatically update your x configuration so that the NVIDIA x driver will be used when you restart x? Any pre-existing x confile will be backed up.  选择 Yes  继续</p>
<hr>
<h4 id="第三步"><a href="#第三步" class="headerlink" title="第三步"></a>第三步</h4><ol>
<li>挂载NVIDIA驱动<code>modprobe nvidia</code></li>
<li>查看驱动是否安装成功：<code>nvidia-smi</code></li>
<li>恢复图形界面 <code>sudo service lightdm start</code></li>
</ol>
<hr>
<h3 id="至此大功告成"><a href="#至此大功告成" class="headerlink" title="至此大功告成"></a>至此大功告成</h3>]]></content>
      <categories>
        <category>system</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Nvidia</tag>
      </tags>
  </entry>
  <entry>
    <title>手把手教你使用hexo搭建个人博客并部署到远端（Mac版)</title>
    <url>/jerryzz668.github.io/2019/12/30/%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0%E8%BF%9C%E7%AB%AF%EF%BC%88Mac%E7%89%88)/</url>
    <content><![CDATA[<p>Hexo+GitHub的好处：<br>全是静态文件，访问速度快；</p>
<a id="more"></a>
<p>免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台；<br>数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行；<br>博客内容可以轻松打包、转移、发布到其它平台；<br>出现问题的话，网上解决这种问题的方案比较多,等等；</p>
<hr>
<h2 id="下面开始搭建"><a href="#下面开始搭建" class="headerlink" title="下面开始搭建"></a>下面开始搭建</h2><ol>
<li>安装<a href="https://nodejs.org/en/" target="_blank" rel="noopener">node.js</a>（网址：nodejs.org）下载LTS长期支持版，安装。</li>
<li>打开terminal,切换到root用户，输入<code>$ sudo su</code>，输入密码，</li>
<li>安装cnpm包管理器，输入  (-g 表示全局安装)<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ npm install -g cnpm --registry=https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure></li>
<li>用cnpm安装hexo，输入  （hexo -v 查看当前版本)<figure class="highlight avrasm"><table><tr><td class="code"><pre><span class="line">$ cnpm install -g hexo-<span class="keyword">cli</span></span><br></pre></td></tr></table></figure></li>
<li>新建名为blog的文件夹，所有blog相关的文件都在这个文件里面，如果出错，直接干掉这个文件夹就可以了。 输入<code>$  mkdir blog</code></li>
<li>进入blog文件夹  输入<code>$ cd blog/</code>,使用hexo进行安装博客,输入<code>$ sudo hexo init</code></li>
<li>安装完成，进行启动 输入<code>$ hexo s</code>（s代表server）出现Hexo is running at http：//localhost：4000. 复制网址到网页，你的博客已经出现了，默认为你创建了一篇博客。</li>
<li>ctrl+c中断当前操作，新建一篇你自己的博客 输入（n代表new)<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo n “我的第一篇博客”</span><br></pre></td></tr></table></figure></li>
<li>进入新建的博客,输入<code>$ cd source/_posts/</code>,编辑博客,输入<code>$ vim 我的第一篇博客.md</code>,将以下基于markdown格式的文本复制到你的博客中，或者输入i，自己将一下内容手敲进去：<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 第一章</span></span><br><span class="line"><span class="string">内容</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment">##第二章</span></span><br><span class="line"><span class="string">内容</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment">##参考文献</span></span><br><span class="line"><span class="string">jerryzz668.github.io</span></span><br></pre></td></tr></table></figure>
先esc然后:wq（w是保存，q是退出）</li>
<li>后退两层目录到blog路径下,输入<code>$ cd ../..</code></li>
<li>清理一下,输入<code>$ hexo clean</code></li>
<li>编辑完内容生成博客,输入<code>$ hexo g</code>（g代表generate）</li>
<li>启动刚生成的博客,输入<code>$ hexo s</code>,同样复制本地4000端口网址到网页，刚写好的博客就成功生成了</li>
<li>ctrl+c中断当前操作。博客总不能放在本地使用吧，现在我们将他部署到远端公开使用，有一个免费的平台Github（大型同性交友网站），去注册一个GitHub账号（名字起的好听点，以后会用到）</li>
<li>登陆GitHub，点击头像旁边的‘+’，new 一个Repository，然后在owner后面的repository name下填写你的用户名.github.io （注意：此处的你的昵称一定要填写正确，比如我的是jerryzz668.github.io, 以后就用这个网址来访问你的博客），description中可以填写 我的hexo博客，然后create repository，页面不要关闭，一会步骤17会用到。</li>
<li>回到terminal,在blog路径下安装一个git部署插件,输入<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cnpm install —save hexo-deployer-git</span><br></pre></td></tr></table></figure></li>
<li>设置config文件,输入<code>$ vim _config.yml</code>,直接到文件最底部，输入i进行编辑，将下面三行敲进去(注意：每个冒号后都有一个空格)<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">type</span>: git</span><br><span class="line">repo: 将刚才步骤15网页中的SSH后面的网址复制到这里</span><br><span class="line">branch: master</span><br></pre></td></tr></table></figure>
保存退出（方法同步骤9）</li>
<li>config编辑完成,下面部署到远端,输入<code>$ hexo d</code>（d代表deploy）,输入GitHub账号和密码，现在就是在往远端进行推送，完成之后，刷新github中你刚create 的repository，发现多了很多内容。</li>
<li>现在复制<code>$ 你的用户名.github.io</code>到网址，就可以看到刚刚你自己编写的博客了，恭喜你，部署成功，终于拥有了自己的博客。</li>
<li>小伙伴可能觉得这个主题不太喜欢，接下来自定义更换主题，推荐这个网址的主题 github.com/litten/hexo-theme-yilia, 输入<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/litten/hexo-theme-yilia.git theme/yilia</span><br></pre></td></tr></table></figure></li>
<li>配置config文件,输入<code>$ vim _config.yml</code>,找到theme：后面的主题改成yilia，保存退出同步骤9. 输入<code>$ hexo clean</code>,再输入<code>$ hexo g</code>（g代表generate),再输入<code>$ hexo s</code>（s代表server）,4000端口刷新</li>
<li>ctrl+c退出，然后推送到远端，输入<code>$ hexo d</code>（d代表deploy）,然后去<code>$ 你的用户名.github.io</code> 这个网址，刷新，即可看到更改好的主题。</li>
</ol>
<hr>
<h2 id="至此大功告成，快去享受你的博客吧"><a href="#至此大功告成，快去享受你的博客吧" class="headerlink" title="至此大功告成，快去享受你的博客吧"></a>至此大功告成，快去享受你的博客吧</h2>]]></content>
      <categories>
        <category>technology</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>blog</tag>
      </tags>
  </entry>
</search>
